{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9486652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLA JIT: ENABLED\n",
      "GPU detected and configured for training: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DATASET = \"KTH\"\n",
    "\n",
    "MODEL_ARCH = \"64\"\n",
    "USE_CONTEXT = True\n",
    "USE_FOVEA = True\n",
    "\n",
    "N_SAMPLES = 40000\n",
    "PERPLEXITY = 400\n",
    "MAX_ITER = 1000\n",
    "\n",
    "EXPERIMENT = \"both64_500_64_ctx64x64_fov32x32\"\n",
    "OUTPUT_DIR = f\"{DATASET}_results/{EXPERIMENT}\"\n",
    "\n",
    "CONTEXT_SHAPE = (64, 64, 1)\n",
    "FOVEA_SHAPE = (32, 32, 1)\n",
    "\n",
    "\n",
    "CLASS_NAMES = [\"boxing\", \"handclapping\", \"handwaving\", \"walking\"] if DATASET != \"UCF11\" else [\"basketball\",\"biking\",\"diving\",\"golf_swing\",\"horse_riding\",\"soccer_juggling\",\"swing\",\"tennis_swing\",\"trampoline_jumping\",\"volleyball_spiking\",\"walking\"]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "\n",
    "ENABLE_XLA_JIT      = True\n",
    "CACHE_TO_DISK       = True\n",
    "PREFETCH_TO_DEVICE  = True\n",
    "SHUFFLE_BUFFER      = 30000\n",
    "\n",
    "try:\n",
    "    tf.config.optimizer.set_jit(ENABLE_XLA_JIT)\n",
    "    print(\"XLA JIT:\", \"ENABLED\" if ENABLE_XLA_JIT else \"DISABLED\")\n",
    "except Exception as e:\n",
    "    print(\"Could not set XLA JIT:\", e)\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(\"GPU detected and configured for training:\", gpus[0])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1381d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "if MODEL_ARCH == \"64\":\n",
    "    CHANNEL1_SIZE, CHANNEL2_SIZE, DENSE1_SIZE, DENSE2_SIZE = 16, 16, 10, 16\n",
    "elif MODEL_ARCH == \"32\":\n",
    "    CHANNEL1_SIZE, CHANNEL2_SIZE, DENSE1_SIZE, DENSE2_SIZE = 16, 32, 20, 32\n",
    "else:\n",
    "    CHANNEL1_SIZE, CHANNEL2_SIZE, DENSE1_SIZE, DENSE2_SIZE = 8, 16, 8, 8\n",
    "\n",
    "def build_branch(input_shape, name_prefix=\"\"):\n",
    "    inp = keras.Input(shape=input_shape, name=f\"{name_prefix}input\")\n",
    "    x = layers.Conv2D(CHANNEL1_SIZE, (3, 3), kernel_initializer=\"he_normal\", bias_initializer=\"zeros\")(inp)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(CHANNEL2_SIZE, (3, 3), kernel_initializer=\"he_normal\", bias_initializer=\"zeros\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return inp, x\n",
    "\n",
    "def build_model(use_context=True, use_fovea=True):\n",
    "    inputs, branches = [], []\n",
    "    if use_context:\n",
    "        ci, co = build_branch(CONTEXT_SHAPE, \"context_\")\n",
    "        inputs.append(ci)\n",
    "        branches.append(co)\n",
    "    if use_fovea:\n",
    "        fi, fo = build_branch(FOVEA_SHAPE, \"fovea_\")\n",
    "        inputs.append(fi)\n",
    "        branches.append(fo)\n",
    "    fused = branches[0] if len(branches) == 1 else layers.Concatenate()(branches)\n",
    "    z = layers.Dense(DENSE1_SIZE, kernel_initializer=\"he_normal\", bias_initializer=\"zeros\")(fused)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    z = layers.Dense(DENSE2_SIZE, name=\"embedding\", kernel_initializer=\"he_normal\", bias_initializer=\"zeros\")(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\", kernel_initializer=\"he_normal\", bias_initializer=\"zeros\")(z)\n",
    "    return Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "def make_tf_dataset(features_paths, labels, batch_size, shuffle=False, drop_remainder=False):\n",
    "    feat_elems = {k: tf.constant(v) for k, v in features_paths.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((feat_elems, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(min(30000, len(labels)))\n",
    "    ctx_h, ctx_w, _ = CONTEXT_SHAPE\n",
    "    fov_h, fov_w, _ = FOVEA_SHAPE\n",
    "    def _loader(features, label):\n",
    "        out_feats = {}\n",
    "        if \"context_input\" in features:\n",
    "            x = tf.io.read_file(features[\"context_input\"])\n",
    "            x = tf.io.decode_png(x, channels=1)\n",
    "            x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "            x = tf.image.resize(x, [ctx_h, ctx_w])\n",
    "            x = tf.ensure_shape(x, CONTEXT_SHAPE)\n",
    "            out_feats[\"context_input\"] = x\n",
    "        if \"fovea_input\" in features:\n",
    "            x = tf.io.read_file(features[\"fovea_input\"])\n",
    "            x = tf.io.decode_png(x, channels=1)\n",
    "            x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "            x = tf.image.resize(x, [fov_h, fov_w])\n",
    "            x = tf.ensure_shape(x, FOVEA_SHAPE)\n",
    "            out_feats[\"fovea_input\"] = x\n",
    "        return out_feats, label\n",
    "    ds = ds.map(_loader, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "CHECKPOINTS_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")\n",
    "PLOTS_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "TEST_DATA_DIR = os.path.join(OUTPUT_DIR, \"test_data\")\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 30 variables. \n",
      "W0000 00:00:1754873141.637436  938274 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: KTH_results/both64_500_64_ctx64x64_fov32x32/plots/tsne_fold1_s40000_p400_i1000.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "fold_id = 1\n",
    "\n",
    "data = np.load(os.path.join(TEST_DATA_DIR, f\"fold_{fold_id}_test_data.npz\"), allow_pickle=True)\n",
    "y = data[\"y\"].astype(np.int32)\n",
    "features = {}\n",
    "use_ctx = \"X_context_paths\" in data.files\n",
    "use_fov = \"X_fovea_paths\" in data.files\n",
    "\n",
    "indices = np.random.choice(len(y), size=N_SAMPLES, replace=False)\n",
    "\n",
    "\n",
    "y = y[indices]\n",
    "if \"X_context_paths\" in data.files:\n",
    "    features[\"context_input\"] = data[\"X_context_paths\"][indices].astype(str).tolist()\n",
    "if \"X_fovea_paths\" in data.files:\n",
    "    features[\"fovea_input\"] = data[\"X_fovea_paths\"][indices].astype(str).tolist()\n",
    "\n",
    "ds = make_tf_dataset(features, y, batch_size=BATCH_SIZE, shuffle=False, drop_remainder=False)\n",
    "\n",
    "model = build_model(use_ctx, use_fov)\n",
    "model.compile(optimizer=\"adam\",\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=[\"accuracy\"])    \n",
    "ckpt_path = os.path.join(CHECKPOINTS_DIR, f\"fold_{fold_id}_best_model.keras\")\n",
    "model.load_weights(ckpt_path)\n",
    "embed_model = Model(inputs=model.inputs, outputs=model.get_layer(\"embedding\").output)\n",
    "embeddings = embed_model.predict(ds, verbose=0)\n",
    "\n",
    "Z = TSNE(n_components=2,\n",
    "         init=\"pca\",\n",
    "         perplexity=PERPLEXITY,\n",
    "         learning_rate=\"auto\",\n",
    "         max_iter=MAX_ITER,\n",
    "         random_state=42).fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "palette = [\"r\",\"g\",\"b\",\"c\",\"m\",\"y\",\"k\",\"tab:orange\",\"tab:purple\",\"tab:brown\",\"tab:pink\",\"tab:gray\",\"tab:olive\",\"tab:cyan\"]\n",
    "for cls_idx in range(NUM_CLASSES):\n",
    "    mask = y == cls_idx\n",
    "    if np.any(mask):\n",
    "        plt.scatter(Z[mask, 0], Z[mask, 1], s=18, alpha=0.9, label=CLASS_NAMES[cls_idx] if cls_idx < len(CLASS_NAMES) else f\"C{cls_idx+1}\", c=palette[cls_idx % len(palette)])\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(loc=\"best\", title=\"Classes\")\n",
    "plt.tight_layout()\n",
    "save_path = os.path.join(PLOTS_DIR, f\"tsne_fold{fold_id}_s{N_SAMPLES}_p{PERPLEXITY}_i{MAX_ITER}.png\")\n",
    "plt.savefig(save_path, dpi=200)\n",
    "plt.close()\n",
    "print(\"Saved:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
